import os
import numpy as np
import time
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import models.bayesian.simple_cnn as simple_cnn
import json

# MALWARE IMPORTS
from utils.utils import load_parameters, stack_tensors, select_mals
from datasets.datasets import load_data
from inner_maximizers.inner_maximizers import inner_maximizer
from moped.util import entropy, accuracy_vs_uncertainty, eval_avu
from avuc.avuc_loss import AvULoss

model_names = "SCNN"

print(model_names)

class Params():
    def __init__(self, parameters):
        self.print_freq = int(parameters["general"]["log_interval"])
    
        self.training_method = parameters["general"]["training_method"]
        self.evasion_method = parameters["general"]["evasion_method"]
        self.experiment_suffix = parameters["general"]["experiment_suffix"]
        self.experiment_name = "[training_%s_evasion_%s]_%s" % (self.training_method, self.evasion_method,
                                                            self.experiment_suffix)
        self.model_name = "[training_%s_evasion_%s]_%s" % (self.training_method, self.training_method,
                                                            self.experiment_suffix)
        print("Training Method:%s, Evasion Method:%s" % (self.training_method, self.evasion_method))

        self.start_epoch = 0
        self.epochs = int(parameters["hyperparam"]["ff_num_epochs"])
        self.evasion_iterations = int(parameters['hyperparam']['evasion_iterations'])
        self.lr = float(parameters["hyperparam"]["ff_learning_rate"])

        self.train_model_from_scratch = eval(parameters['general']['train_model_from_scratch'])
        self.load_model_weights = eval(parameters['general']['load_model_weights'])
        self.model_weights_path = parameters['general']['model_weights_path']
            
        self.log_dir = "output/metrics"
        self.save_dir = "output/trained_models"

        self.num_monte_carlo = int(parameters['hyperparam']['num_monte_carlo'])
        self.epsilon = float(parameters['hyperparam']['epsilon'])
        self.seed_val = int(parameters["general"]["seed"])

def main():
    # Malware
    parameters = load_parameters("parameters.ini")

    global params, best_prec1, opt_th, best_avu, beta
    params = Params(parameters)
    optimal_threshold = 1.0
    opt_th = optimal_threshold
    best_prec1 = 0
    best_avu = 0
    beta = 3.0

    torch.manual_seed(params.seed_val)
    np.random.seed(params.seed_val)

    # Check the save_dir exists or not
    if not os.path.exists("output"):
        os.makedirs("output")
    if not os.path.exists(params.save_dir):
        os.makedirs(params.save_dir)

    # model = torch.nn.DataParallel(simple_cnn.SCNN())

    model = simple_cnn.SCNN()

    if torch.cuda.is_available():
        model.cuda()
    else:
        model.cpu()

    cudnn.benchmark = True

    derived_results_dir = os.path.join(params.log_dir, 'derived_results')
    if not os.path.exists(derived_results_dir):
        os.makedirs(derived_results_dir)
    results_dir = os.path.join(params.log_dir, 'normal_results')
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)

    train_dataloader_dict, valid_dataloader_dict, test_dataloader_dict, sizes = load_data(parameters)

    global len_trainset, len_valset, len_testset
    len_trainset = sizes["train_malicious"] + sizes["train_benign"]
    len_valset = sizes["valid_malicious"] + sizes["valid_benign"]
    len_testset = sizes["test_malicious"] + sizes["test_benign"]

    if not os.path.exists(params.save_dir):
        os.makedirs(params.save_dir)

    if torch.cuda.is_available():
        criterion = nn.CrossEntropyLoss(reduction='none').cuda()
        avu_criterion = AvULoss().cuda()
    else:
        criterion = nn.CrossEntropyLoss(reduction='none').cpu()
        avu_criterion = AvULoss().cpu()

    if params.train_model_from_scratch:
        for epoch in range(params.start_epoch, params.epochs):
            lr = params.lr

            # if (epoch >= 80 and epoch < 120):
            #     lr = 0.1 * params.lr
            # elif (epoch >= 120 and epoch < 160):
            #     lr = 0.01 * params.lr
            # elif (epoch >= 160 and epoch < 180):
            #     lr = 0.001 * params.lr
            # elif (epoch >= 180):
            #     lr = 0.0005 * params.lr

            optimizer = torch.optim.Adam(model.parameters(), lr)
            
            # train for one epoch
            print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))
            train(train_dataloader_dict, model, criterion, avu_criterion, optimizer, epoch)
            prec1 = validate(valid_dataloader_dict, model, criterion, avu_criterion, epoch)
            is_best = prec1 > best_prec1
            best_prec1 = max(prec1, best_prec1)

            if epoch >= 0:
                if is_best:
                    print('\n******Checkpoint***********\n')
                    print('epoch :', epoch + 1)
                    print('best_prec1 :', best_prec1)
                    print('\n***************************\n')
                    save_checkpoint(
                        {
                            'epoch': epoch + 1,
                            'state_dict': model.state_dict(),
                            'best_prec1': best_prec1,
                        },
                        is_best,
                        filename=os.path.join(
                            params.save_dir,
                            '{}-model.pth'.format(params.model_name)))

    checkpoint_file = params.save_dir + '/{}-model.pth'.format(
        params.model_name)
    if torch.cuda.is_available():
        checkpoint = torch.load(checkpoint_file)
    else:
        checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))
    
    model.load_state_dict(checkpoint['state_dict'])

    #Evaluate on test dataset
    metrics = test(model, test_dataloader_dict, criterion)
    print('\n******METRICS***********\n')
    print(metrics)
    print('\n************************\n')

    with open(os.path.join(results_dir, params.experiment_name + ".json"), "w") as result_file:
        json.dump(metrics, result_file)

    derived_metrics(metrics, derived_results_dir)

def train(train_dataloader_dict, model, criterion, avu_criterion, optimizer, epoch):
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    avg_unc = AverageMeter()

    print('Train epsilon: ', params.epsilon)

    # switch to train mode
    model.train()

    preds_list = []
    labels_list = []
    unc_list = []
    end = time.time()
    for i, ((bon_x, bon_y), (mal_x, mal_y)) in enumerate(
            zip(train_dataloader_dict["benign"], train_dataloader_dict["malicious"])):
        # measure data loading time
        data_time.update(time.time() - end)
        
        mal_x1 = inner_maximizer(mal_x, mal_y, model, criterion, iterations=params.evasion_iterations, method=params.training_method, eps=params.epsilon, is_report_loss_diff=(i % params.print_freq) == 0)

        res = select_mals(mal_x, mal_x1, model, epoch, i)

        if torch.cuda.is_available():
            input_var = stack_tensors(bon_x, res).cuda()
            target_var = stack_tensors(bon_y, mal_y).cuda()
        else:
            input_var = stack_tensors(bon_x, res)
            target_var = stack_tensors(bon_y, mal_y)

        optimizer.zero_grad()

        output, kl = model(input_var)
        probs_ = torch.nn.functional.softmax(output, dim=1)
        probs = probs_.data.cpu().numpy()

        pred_entropy = entropy(probs)
        unc = np.mean(pred_entropy, axis=0)
        preds = np.argmax(probs, axis=-1)
        preds_list.append(preds)
        labels_list.append(target_var.cpu().data.numpy())
        unc_list.append(pred_entropy)

        AvU = accuracy_vs_uncertainty(np.array(preds),
                                           np.array(target_var.cpu().data.numpy()),
                                           np.array(pred_entropy), opt_th)

        cross_entropy_loss = criterion(output, target_var).mean()
        scaled_kl = kl.data / len_trainset
        elbo_loss = cross_entropy_loss + scaled_kl
        avu_loss = beta * avu_criterion(output, target_var, opt_th, type=0)
        loss = cross_entropy_loss + scaled_kl + avu_loss

        # compute gradient and do SGD step
        loss.backward()
        optimizer.step()

        output = output.float()
        loss = loss.float()
        # measure accuracy and record loss
        prec1 = accuracy(output.data, target_var)[0]
        losses.update(loss.item(), input_var.size(0))
        top1.update(prec1.item(), input_var.size(0))
        avg_unc.update(unc, input_var.size(0))

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if i % params.print_freq == 0:
            print('Epoch: [{0}][{1}/{2}]\t'
                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
                  'Avg_Unc {avg_unc.val:.3f} ({avg_unc.avg:.3f})'.format(
                      epoch,
                      i,
                      (train_dataloader_dict["benign"].__len__() + train_dataloader_dict["malicious"].__len__())/2,
                      batch_time=batch_time,
                      data_time=data_time,
                      loss=losses,
                      top1=top1,
                      avg_unc=avg_unc))

    print('\nTRAINING EPOCH: ', epoch)
    preds = np.hstack(np.asarray(preds_list))
    labels = np.hstack(np.asarray(labels_list))
    unc_ = np.hstack(np.asarray(unc_list))
    unc_correct = np.take(unc_, np.where(preds == labels))
    unc_incorrect = np.take(unc_, np.where(preds != labels))
    print('avg unc correct preds: ',
          np.mean(np.take(unc_, np.where(preds == labels)), axis=1))
    print('avg unc incorrect preds: ',
          np.mean(np.take(unc_, np.where(preds != labels)), axis=1))
    print()

def validate(valid_dataloader_dict, model, criterion, avu_criterion, epoch):
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    avg_unc = AverageMeter()
    global opt_th

    # switch to evaluate mode
    model.eval()

    preds_list = []
    labels_list = []
    unc_list = []
    end = time.time()
    with torch.no_grad():
        for i, ((bon_x, bon_y), (mal_x, mal_y)) in enumerate(
            zip(valid_dataloader_dict["benign"], valid_dataloader_dict["malicious"])):
            
            if torch.cuda.is_available():
                input_var = stack_tensors(bon_x, mal_x).cuda()
                target_var = stack_tensors(bon_y, mal_y).cuda()
            else:
                input_var = stack_tensors(bon_x, mal_x)
                target_var = stack_tensors(bon_y, mal_y)
            
            output, kl = model(input_var)
            probs_ = torch.nn.functional.softmax(output, dim=1)
            probs = probs_.data.cpu().numpy()

            pred_entropy = entropy(probs)
            unc = np.mean(pred_entropy, axis=0)
            preds = np.argmax(probs, axis=-1)
            preds_list.append(preds)
            labels_list.append(target_var.cpu().data.numpy())
            unc_list.append(pred_entropy)

            AvU = accuracy_vs_uncertainty(
                np.array(preds), np.array(target_var.cpu().data.numpy()),
                np.array(pred_entropy), opt_th)

            cross_entropy_loss = criterion(output, target_var).mean()
            scaled_kl = kl.data / len_trainset
            elbo_loss = cross_entropy_loss + scaled_kl
            avu_loss = beta * avu_criterion(output, target_var, opt_th, type=0)
            loss = cross_entropy_loss + scaled_kl + avu_loss

            output = output.float()
            loss = loss.float()

            # measure accuracy and record loss
            prec1 = accuracy(output.data, target_var)[0]
            losses.update(loss.item(), input_var.size(0))
            top1.update(prec1.item(), input_var.size(0))
            avg_unc.update(unc, input_var.size(0))

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if i % params.print_freq == 0:
                print('Test: [{0}/{1}]\t'
                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
                      'Avg_Unc {avg_unc.val:.3f} ({avg_unc.avg:.3f})'.format(
                          i,
                          (valid_dataloader_dict["benign"].__len__() + valid_dataloader_dict["malicious"].__len__())/2,
                          batch_time=batch_time,
                          loss=losses,
                          top1=top1,
                          avg_unc=avg_unc))

        preds = np.hstack(np.asarray(preds_list))
        labels = np.hstack(np.asarray(labels_list))
        unc_ = np.hstack(np.asarray(unc_list))
        avu_th, unc_th = eval_avu(preds, labels, unc_)
        print('max AvU: ', np.amax(avu_th))
        unc_correct = np.take(unc_, np.where(preds == labels))
        unc_incorrect = np.take(unc_, np.where(preds != labels))
        print('avg unc correct preds: ',
              np.mean(np.take(unc_, np.where(preds == labels)), axis=1))
        print('avg unc incorrect preds: ',
              np.mean(np.take(unc_, np.where(preds != labels)), axis=1))
        '''
        print('unc @max AvU: ', unc_th[np.argmax(avu_th)])
        print('avg unc: ', np.mean(unc_, axis=0))
        print('avg unc: ', np.mean(unc_th, axis=0))
        print('min unc: ', np.amin(unc_))
        print('max unc: ', np.amax(unc_))
        '''
        if epoch <= 5:
            opt_th = (np.mean(unc_correct, axis=1) +
                      np.mean(unc_incorrect, axis=1)) / 2

    print('opt_th: ', opt_th)
    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))
    return top1.avg

def evaluate(model, test_dataloader_dict, criterion, category = "benign", is_evade = False,
                evade_method = 'dfgsm_k', epsilon = 0.2):
    total_correct = 0
    total = 0
    evasion_mode = ""

    pred_probs_mc = []
    if is_evade:
        print(f"Evasion using {evade_method}")
        print(f"Test epsilon: {epsilon}")

    dataloader = test_dataloader_dict[category]

    for batch_idx, (data, target) in enumerate(dataloader):
        # print('Batch idx {}, data shape {}, target shape {}'.format(batch_idx, data.shape, target.shape))
        if is_evade:
            data = inner_maximizer(data, target, model, criterion, iterations=params.evasion_iterations, method=evade_method, eps=epsilon)
            evasion_mode = "(evasion using %s)" % evade_method

        if torch.cuda.is_available():
            data, target = data.cuda(), target.cuda()
        else:
            data, target = data.cpu(), target.cpu()
        for mc_run in range(params.num_monte_carlo):
            model.eval()
            output, _ = model.forward(data)
            pred_probs = torch.nn.functional.softmax(output, dim=1)
            pred_probs_mc.append(pred_probs.cpu().data.numpy())

        target_labels = target.cpu().data.numpy()
        pred_mean = np.mean(pred_probs_mc, axis=0)
        Y_pred = np.argmax(pred_mean, axis=1)
        pred_final = (Y_pred == target_labels)
        correct = (pred_final).sum()
        total_correct += correct
        total += len(pred_final)

    print("Test set for {} {}: Accuracy: {:.2f}%".format(
        category, evasion_mode, total_correct * 100. / total))

    return total_correct, total

def test(model, test_dataloader_dict, criterion):

    # test for accuracy and loss
    bon_total_correct, bon_total = evaluate(model, test_dataloader_dict, 
        criterion, category = "benign", 
        is_evade = False)

    mal_total_correct, mal_total = evaluate(model, test_dataloader_dict, 
        criterion, category = "malicious", 
        is_evade = False)

    # test for evasion on malicious sample
    evade_mal_total_correct, evade_mal_total = evaluate(model, test_dataloader_dict, 
        criterion, category = "malicious", 
        is_evade = True, evade_method = params.evasion_method, epsilon = params.epsilon)

    total_correct = bon_total_correct + mal_total_correct
    total = bon_total + mal_total

    print("Test set overall: Accuracy: {:.2f}%".format(
            total_correct * 100. / total))

    metrics = {
        "mal": {
            "total_correct": mal_total_correct.item(),
            "total": mal_total,
            "evasion": {
                "total_correct": evade_mal_total_correct.item(),
                "total": evade_mal_total
            }
        },
        "bon": {
            "total_correct": bon_total_correct.item(),
            "total": bon_total
        }
    }

    return metrics

def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
    """
    Save the training model
    """
    torch.save(state, filename)


class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def accuracy(output, target, topk=(1, )):
    """Computes the precision@k for the specified values of k"""
    maxk = max(topk)
    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res

def derived_metrics(metrics, derived_results_dir):
    print("Saving dervived metrics!")

    #natural metrics calculation
    nat_tp = metrics['mal']['total_correct']
    nat_fp =  metrics['bon']['total'] -  metrics['bon']['total_correct']
    nat_fn =  metrics['mal']['total'] -  metrics['mal']['total_correct']
    nat_tn =  metrics['bon']['total_correct']
    nat_TPR = nat_tp / (nat_tp + nat_fn)
    nat_FPR = nat_fp / (nat_fp + nat_tn)
    nat_F1 = (2* nat_tp)/ (2*nat_tp + nat_fp + nat_fn)
    nat_acc = (nat_tp + nat_tn)/(nat_tp + nat_tn + nat_fn + nat_fp)
    nat_pre = (nat_tp)/(nat_tp+nat_fp) 

    #evasion metrics calculation
    ev_tp =  metrics['mal']['evasion']['total_correct']
    ev_fp = metrics['bon']['total'] -  metrics['bon']['total_correct']
    ev_fn = metrics['mal']['evasion']['total'] - metrics['mal']['evasion']['total_correct']
    ev_tn = metrics['bon']['total_correct']
    ev_TPR = ev_tp / (ev_tp + ev_fn)
    ev_FPR = ev_fp / (ev_fp + ev_tn)
    ev_F1 = (2* ev_tp)/ (2*ev_tp + ev_fp + ev_fn)
    ev_acc = (ev_tp + ev_tn)/(ev_tp + ev_tn + ev_fn + ev_fp)

    rr_sensitivity = ev_TPR/nat_TPR

    #evasion rate
    ev_rate = (metrics['mal']['evasion']['total'] - metrics['mal']['evasion']['total_correct'])/metrics['mal']['evasion']['total']
    
    #misclassification rate
    misclass_rate = 1 - ev_acc

    derived_metrics = {
        "natural": {
            "Nat_TP": nat_tp,
            "Nat_FP": nat_fp, 
            "Nat_FN": nat_fn,
            "Nat_TN": nat_tn,
            "Nat_accuracy": nat_acc,
            "Nat_TPR": nat_TPR,
            "Nat_FPR": nat_FPR,
            "Nat_F1": nat_F1,
            "Nat_Precision": nat_pre
        },
        "evasion":{
            "Ev_TP": ev_tp,
            "Ev_FP": ev_fp, 
            "Ev_FN": ev_fn,
            "Ev_TN": ev_tn,
            "Ev_accuracy": ev_acc,
            "Ev_TPR": ev_TPR,
            "Ev_FPR": ev_FPR,
            "Ev_F1": ev_F1
        },
        "robustness_ratio":{
            "RR_sensitivity": rr_sensitivity
        },
        "evasion_rate":{
            "evasion_rate_fnr": ev_rate 
        },
        "misclassification_rate_ev":{
            "misclass_rate_ev": misclass_rate
        }}

    print('\n******DERIVED METRICS***********\n')
    print(derived_metrics)
    print('\n********************************\n')

    with open(os.path.join(derived_results_dir, params.experiment_name + ".json"), "w") as derived_result_file:
        json.dump(derived_metrics, derived_result_file)

if __name__ == '__main__':
    main()
